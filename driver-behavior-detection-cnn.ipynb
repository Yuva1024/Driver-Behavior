{"cells":[{"cell_type":"markdown","id":"7de0a70e","metadata":{"papermill":{"duration":0.025294,"end_time":"2023-10-18T18:26:32.759913","exception":false,"start_time":"2023-10-18T18:26:32.734619","status":"completed"},"tags":[],"id":"7de0a70e"},"source":["![1.jpg](attachment:d82f01cf-25e2-4069-a372-c853a4a51c68.jpg)"]},{"cell_type":"markdown","id":"faefa0e1","metadata":{"papermill":{"duration":0.0223,"end_time":"2023-10-18T18:26:32.805463","exception":false,"start_time":"2023-10-18T18:26:32.783163","status":"completed"},"tags":[],"id":"faefa0e1"},"source":["<p style=\"text-align:center;\"><span style=\"font-size:48px;\"><span style=\"color:navy;\"><span style=\"font-family:cursive;\"> Hello everyone! </span> </span>  </span></p>"]},{"cell_type":"markdown","id":"c69709fe","metadata":{"papermill":{"duration":0.022491,"end_time":"2023-10-18T18:26:32.850229","exception":false,"start_time":"2023-10-18T18:26:32.827738","status":"completed"},"tags":[],"id":"c69709fe"},"source":["# <p style=\"padding:10px;background-color:#fab72f ;margin:0;color:#ffffff;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:500\">Table of Contents üíª</p>"]},{"cell_type":"markdown","id":"04d8c1f1","metadata":{"papermill":{"duration":0.021356,"end_time":"2023-10-18T18:26:32.894256","exception":false,"start_time":"2023-10-18T18:26:32.872900","status":"completed"},"tags":[],"id":"04d8c1f1"},"source":["<div style = 'border : 3px solid non; background-color:#f2f2f2\n","              ;font-family:Times New Roman;\n","              font-size:110%;padding:10px'>\n","\n","\n","* **[1. A Brief Explanation](#1)**\n","\n","  - What is the dataset about?\n","    \n","  - Importance of this topic\n","   \n","   \n","* **[2. About this project](#2)**\n","\n","  - Why CNN?\n","    \n","  - Which structures?\n","    \n","    \n","    \n","* **[3. Import Libraries ](#3)**\n","\n","  - Libraries that we use in this project\n","  \n","  \n","* **[4. Import Dataset ](#4)**  \n","\n","   - Calling the images from directory\n","   \n","   \n","   \n","* **[5. Pre Modeling ](#5)**\n","\n","   - Delete unreadable images\n","    \n","   - Plot random images from each class\n","    \n","   - Split data to train, test, prediction\n","    \n","   - Create dataframes for train, test, prediction\n","    \n","   - Calculate the ratio of images\n","    \n","   - Define hyperparameters\n","    \n","   - Rescale the images\n","    \n","   - Model inputs\n","   \n","   \n","* **[6. AlexNet ](#6)**\n","    \n","   - What is AlexNet?\n","    \n","   - Model Structure\n","    \n","   - Output\n","   \n","   \n","* **[7. VGGNet](#7)**\n","\n","  - What is VGGNet?\n","    \n","  - Model Structure\n","    \n","  - Output\n","    \n","\n","* **[8. ResNet](#8)**\n","\n","  - What is ResNet?\n","  \n","  - Model Structure\n","    \n","  - Output\n","  \n","\n","* **[9. Result](#9)**\n","\n","  - Comparision of (Acc)s and (Loss)s\n","    \n","  - Conclusion\n","    \n","\n","* **[10. Recommendation Topics](#10)**\n","    \n","    - How to connect these models to a Camera?\n","    \n","    - How to build a voice alarm system for a Car?\n","    \n","    \n","\n","\n"]},{"cell_type":"markdown","id":"cb6f3d32","metadata":{"papermill":{"duration":0.021987,"end_time":"2023-10-18T18:26:32.937967","exception":false,"start_time":"2023-10-18T18:26:32.915980","status":"completed"},"tags":[],"id":"cb6f3d32"},"source":["<a id=\"1\"></a>\n","# <p style=\"padding:10px;background-color:#fab72f ;margin:0;color:#ffffff;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:500\">A Brief Explanation </p>"]},{"cell_type":"markdown","id":"61def293","metadata":{"papermill":{"duration":0.022386,"end_time":"2023-10-18T18:26:32.982582","exception":false,"start_time":"2023-10-18T18:26:32.960196","status":"completed"},"tags":[],"id":"61def293"},"source":["<div style = 'border : 3px solid non; background-color:#f2f2f2 ; ;padding:10px'>\n","\n","\n","* **1. What is the dataset about?**\n","\n","  - This dataset is about car driver behavior detection. What does it mean?\n","    Driver behavior detection is a field of research that aims to identify     and analyze the actions of drivers while they are driving. In this dataset we have 5 classes: 1- Safe driving, 2- Talking on phone, 3- Texting on phone, 4- Turning and 5- Other activities\n","   \n","   \n","* **2. Which structures?**\n","    \n","  - There are several types of CNN structures that are used in deep learning-based image analysis. In this project, I use three of them. AlexNet, VGGNet and ResNet. These are just some of the most popular CNN structures used in image analysis tasks. Each structure in CNN has its own unique features and advantages and because of that, if we want to use CNN for a case, we have to use more than one structure for comparing which of these structures are better fitting to our images.\n","    \n"]},{"cell_type":"markdown","id":"d43296e9","metadata":{"papermill":{"duration":0.020936,"end_time":"2023-10-18T18:26:33.025103","exception":false,"start_time":"2023-10-18T18:26:33.004167","status":"completed"},"tags":[],"id":"d43296e9"},"source":["<a id=\"2\"></a>\n","# <p style=\"padding:10px;background-color:#fda628 ;margin:0;color:#ffffff;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:500\">About this project </p>"]},{"cell_type":"markdown","id":"205015d5","metadata":{"papermill":{"duration":0.022434,"end_time":"2023-10-18T18:26:33.069897","exception":false,"start_time":"2023-10-18T18:26:33.047463","status":"completed"},"tags":[],"id":"205015d5"},"source":["<div style = 'border : 3px solid non; background-color:#f2f2f2 ; ;padding:10px'>\n","\n","\n","* **1. Why CNN?**\n","\n","  - CNN (Convolutional Neural Network) is a deep learning-based method that is used for driver behavior detection. CNN is a type of neural network that is designed to recognize patterns in images and videos. CNNs are particularly useful for image classification tasks because they can automatically learn to detect features such as edges, corners, and shapes in images. In driver behavior detection, CNNs are used to analyze video footage of drivers and identify patterns of behavior that may indicate unsafe driving practices.\n","   \n","   \n","* **2. Importance of this topic**\n","    \n","  - Data behavior detection is important because it helps to identify potential cybersecurity threats by tracking user behavior and data access activities. By analyzing both user behavior and data access activities, a behavior analytics tool can create a contextual behavior baseline to help discern behaviors that are normal from those that are not and accurately identify critical data threats.\n","  \n","    \n","  \n","* **Note**\n","    - I just want to show you the performance of CNN on this topic and It's all about the structure of AlexNet, VGGNet and ResNet. In next projects I will consider the other parts of this project that I mentioned on the recommendation part (Part 10)."]},{"cell_type":"markdown","id":"02bba34c","metadata":{"papermill":{"duration":0.022462,"end_time":"2023-10-18T18:26:33.156470","exception":false,"start_time":"2023-10-18T18:26:33.134008","status":"completed"},"tags":[],"id":"02bba34c"},"source":["<a id=\"3\"></a>\n","# <p style=\"padding:10px;background-color:#fda628 ;margin:0;color:#ffffff;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:500\">Import Libraries </p>"]},{"cell_type":"code","execution_count":null,"id":"e77b1bd6","metadata":{"execution":{"iopub.execute_input":"2023-10-18T18:26:33.202491Z","iopub.status.busy":"2023-10-18T18:26:33.202201Z","iopub.status.idle":"2023-10-18T18:26:49.299385Z","shell.execute_reply":"2023-10-18T18:26:49.298654Z"},"papermill":{"duration":16.122788,"end_time":"2023-10-18T18:26:49.301773","exception":false,"start_time":"2023-10-18T18:26:33.178985","status":"completed"},"tags":[],"id":"e77b1bd6"},"outputs":[],"source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n","from numpy import asarray\n","import numpy as np\n","import pandas as pd\n","from PIL import Image\n","import cv2\n","import glob\n","import os\n","import random\n","import subprocess\n","import matplotlib.pyplot as plt\n","from skimage.io import imread\n","from matplotlib.patches import Rectangle\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers, models, Input, Model\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.losses import BinaryCrossentropy"]},{"cell_type":"markdown","id":"2db4fb45","metadata":{"papermill":{"duration":0.021723,"end_time":"2023-10-18T18:26:49.348045","exception":false,"start_time":"2023-10-18T18:26:49.326322","status":"completed"},"tags":[],"id":"2db4fb45"},"source":["<a id=\"4\"></a>\n","# <p style=\"padding:10px;background-color:#fda628 ;margin:0;color:#ffffff;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:500\">Import Dataset </p>"]},{"cell_type":"markdown","id":"cb969795","metadata":{"papermill":{"duration":0.021763,"end_time":"2023-10-18T18:26:49.391748","exception":false,"start_time":"2023-10-18T18:26:49.369985","status":"completed"},"tags":[],"id":"cb969795"},"source":["<div style = 'border : 3px solid non; background-color:#f2f2f2 ; ;padding:10px'>\n","\n","\n","* **Calling the images from directory**\n","\n","  - There is 5 differente folder for each class. We have to call them to the different variables. For this purpose, I create 5 empty lists and with some **for loop** for iterating in folders with **os.listdir**. After that we can use a **if** for add images with **png** or **jpg** formats.\n","   You can see these methods in the cell below.\n"]},{"cell_type":"code","execution_count":null,"id":"f001b339","metadata":{"execution":{"iopub.execute_input":"2023-10-18T18:26:49.438253Z","iopub.status.busy":"2023-10-18T18:26:49.437638Z","iopub.status.idle":"2023-10-18T18:26:50.577487Z","shell.execute_reply":"2023-10-18T18:26:50.576759Z"},"papermill":{"duration":1.179525,"end_time":"2023-10-18T18:26:50.592935","exception":false,"start_time":"2023-10-18T18:26:49.413410","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"f001b339","executionInfo":{"status":"error","timestamp":1732353472944,"user_tz":-330,"elapsed":10,"user":{"displayName":"251208 ASHWINRAJ. R","userId":"16240766670791021495"}},"outputId":"5952a368-93d6-4d54-a0b6-8d281f30a9d4"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/input/revitsone-5class/Revitsone-5classes/other_activities'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-1373234608ac>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mimage_list_turn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mother\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/input/revitsone-5class/Revitsone-5classes/other_activities\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".png\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         image_list_other.append(os.path.join(\"/kaggle/input/revitsone-5class/Revitsone-5classes/other_activities\",\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/revitsone-5class/Revitsone-5classes/other_activities'"]}],"source":["image_list_other = []\n","image_list_safe = []\n","image_list_talking = []\n","image_list_text = []\n","image_list_turn = []\n","\n","for other in os.listdir(\"/kaggle/input/revitsone-5class/Revitsone-5classes/other_activities\"):\n","    if other.endswith(\".png\") or other.endswith(\".jpg\"):\n","        image_list_other.append(os.path.join(\"/kaggle/input/revitsone-5class/Revitsone-5classes/other_activities\",\n","                                             other))\n","        print(os.path.join(\"/kaggle/input/revitsone-5class/Revitsone-5classes/other_activities\", other))\n","\n","for safe in os.listdir(\"/kaggle/input/revitsone-5class/Revitsone-5classes/safe_driving\"):\n","    if safe.endswith(\".png\") or safe.endswith(\".jpg\"):\n","        image_list_safe.append(os.path.join(\"/kaggle/input/revitsone-5class/Revitsone-5classes/safe_driving\",\n","                                            safe))\n","        print(os.path.join(\"/kaggle/input/revitsone-5class/Revitsone-5classes/safe_driving\",\n","                           safe))\n","\n","for talking in os.listdir(\"/kaggle/input/revitsone-5class/Revitsone-5classes/talking_phone\"):\n","    if talking.endswith(\".png\") or talking.endswith(\".jpg\"):\n","        image_list_talking.append(os.path.join(\"/kaggle/input/revitsone-5class/Revitsone-5classes/talking_phone\",\n","                                               talking))\n","        print(os.path.join(\"/kaggle/input/revitsone-5class/Revitsone-5classes/talking_phone\",\n","                           talking))\n","\n","for text in os.listdir(\"/kaggle/input/revitsone-5class/Revitsone-5classes/texting_phone\"):\n","    if text.endswith(\".png\") or text.endswith(\".jpg\"):\n","        image_list_text.append(os.path.join(\"/kaggle/input/revitsone-5class/Revitsone-5classes/texting_phone\",\n","                                            text))\n","        print(os.path.join(\"/kaggle/input/revitsone-5class/Revitsone-5classes/texting_phone\",\n","                           text))\n","\n","for turn in os.listdir(\"/kaggle/input/revitsone-5class/Revitsone-5classes/turning\"):\n","    if turn.endswith(\".png\") or turn.endswith(\".jpg\"):\n","        image_list_turn.append(os.path.join(\"/kaggle/input/revitsone-5class/Revitsone-5classes/turning\",\n","                                            turn))\n","        print(os.path.join(\"/kaggle/input/revitsone-5class/Revitsone-5classes/turning\",\n","                           turn))\n","\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"8Tqgk3ViagG5"},"id":"8Tqgk3ViagG5","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"16ea1ed3","metadata":{"papermill":{"duration":0.043308,"end_time":"2023-10-18T18:26:50.685464","exception":false,"start_time":"2023-10-18T18:26:50.642156","status":"completed"},"tags":[],"id":"16ea1ed3"},"source":["<a id=\"5\"></a>\n","# <p style=\"padding:10px;background-color:#fab72f ;margin:0;color:#ffffff;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:500\">Pre Modeling </p>"]},{"cell_type":"markdown","id":"bde46849","metadata":{"papermill":{"duration":0.038774,"end_time":"2023-10-18T18:26:50.763898","exception":false,"start_time":"2023-10-18T18:26:50.725124","status":"completed"},"tags":[],"id":"bde46849"},"source":["<div style = 'border : 3px solid non; background-color:#f2f2f2 ; ;padding:10px'>\n","\n","\n","* **1. Delete unreadable images**\n","\n","  - Sometimes we have some unreadable images that machine cannot read these images. There are two ways for handling these images. 1- Finding these images on our own way and exploring in the dataset if the dataset has little images (like this case). 2- We can create some functions that allow machine to iterate in images for finding out that which images are readable or not.\n","   \n","   \n"]},{"cell_type":"code","execution_count":null,"id":"9ece9508","metadata":{"execution":{"iopub.execute_input":"2023-10-18T18:26:50.844226Z","iopub.status.busy":"2023-10-18T18:26:50.843389Z","iopub.status.idle":"2023-10-18T18:26:50.849217Z","shell.execute_reply":"2023-10-18T18:26:50.848509Z"},"papermill":{"duration":0.047565,"end_time":"2023-10-18T18:26:50.850747","exception":false,"start_time":"2023-10-18T18:26:50.803182","status":"completed"},"tags":[],"id":"9ece9508"},"outputs":[],"source":["image_list_other.remove('/kaggle/input/revitsone-5class/Revitsone-5classes/other_activities/img_79.jpg')\n","image_list_other.remove('/kaggle/input/revitsone-5class/Revitsone-5classes/other_activities/img_4664.jpg')\n","image_list_other.remove('/kaggle/input/revitsone-5class/Revitsone-5classes/other_activities/img_7973.jpg')\n","image_list_other.remove('/kaggle/input/revitsone-5class/Revitsone-5classes/other_activities/img_13318.jpg')\n","image_list_other.remove('/kaggle/input/revitsone-5class/Revitsone-5classes/other_activities/img_13396.jpg')\n","image_list_other.remove('/kaggle/input/revitsone-5class/Revitsone-5classes/other_activities/img_13541.jpg')\n","image_list_other.remove('/kaggle/input/revitsone-5class/Revitsone-5classes/other_activities/img_13625.jpg')\n","image_list_other.remove('/kaggle/input/revitsone-5class/Revitsone-5classes/other_activities/img_20398.jpg')\n","image_list_other.remove('/kaggle/input/revitsone-5class/Revitsone-5classes/other_activities/img_22266.jpg')\n","\n","image_list_turn.remove('/kaggle/input/revitsone-5class/Revitsone-5classes/turning/img_8771.jpg')\n","image_list_turn.remove('/kaggle/input/revitsone-5class/Revitsone-5classes/turning/img_62337.jpg')\n","image_list_turn.remove('/kaggle/input/revitsone-5class/Revitsone-5classes/turning/img_67523.jpg')\n","image_list_turn.remove('/kaggle/input/revitsone-5class/Revitsone-5classes/turning/img_70552.jpg')\n","image_list_turn.remove('/kaggle/input/revitsone-5class/Revitsone-5classes/turning/img_84605.jpg')\n","image_list_turn.remove('/kaggle/input/revitsone-5class/Revitsone-5classes/turning/img_101434.jpg')"]},{"cell_type":"code","execution_count":null,"id":"03382930","metadata":{"execution":{"iopub.execute_input":"2023-10-18T18:26:50.930472Z","iopub.status.busy":"2023-10-18T18:26:50.929789Z","iopub.status.idle":"2023-10-18T18:26:50.933504Z","shell.execute_reply":"2023-10-18T18:26:50.932765Z"},"papermill":{"duration":0.045101,"end_time":"2023-10-18T18:26:50.934937","exception":false,"start_time":"2023-10-18T18:26:50.889836","status":"completed"},"tags":[],"id":"03382930"},"outputs":[],"source":["font = {'family':'Times New Roman','color':'#1f211f'}\n","background_color = '#fab72f'"]},{"cell_type":"markdown","id":"4dea4ce5","metadata":{"papermill":{"duration":0.039605,"end_time":"2023-10-18T18:26:51.014256","exception":false,"start_time":"2023-10-18T18:26:50.974651","status":"completed"},"tags":[],"id":"4dea4ce5"},"source":["<div style = 'border : 3px solid non; background-color:#f2f2f2 ; ;padding:10px'>\n","\n","\n","* **2. Plot random images from each class**\n","\n","  - This part is for better understanding the images that we have.\n","   \n","   \n"]},{"cell_type":"code","execution_count":null,"id":"c93d9d68","metadata":{"execution":{"iopub.execute_input":"2023-10-18T18:26:51.098979Z","iopub.status.busy":"2023-10-18T18:26:51.098176Z","iopub.status.idle":"2023-10-18T18:26:52.129512Z","shell.execute_reply":"2023-10-18T18:26:52.128899Z"},"papermill":{"duration":1.092539,"end_time":"2023-10-18T18:26:52.148502","exception":false,"start_time":"2023-10-18T18:26:51.055963","status":"completed"},"tags":[],"id":"c93d9d68"},"outputs":[],"source":["plt.figure(1, figsize=(15, 9))\n","plt.axis('off')\n","n = 0\n","for i in range(4):\n","\n","    n += 1\n","    random_img = random.choice(image_list_talking)\n","    imgs = imread(random_img)\n","    plt.suptitle(\"Random images of people who talk with their phone\",\n","                 fontdict = font, fontsize=25\n","                 ,backgroundcolor= background_color)\n","    plt.subplot(2,2,n)\n","    plt.imshow(imgs)\n","\n","plt.show()"]},{"cell_type":"code","source":[],"metadata":{"id":"oduN1nQePo4d"},"id":"oduN1nQePo4d","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"abc2b7ad","metadata":{"execution":{"iopub.execute_input":"2023-10-18T18:26:52.296775Z","iopub.status.busy":"2023-10-18T18:26:52.296436Z","iopub.status.idle":"2023-10-18T18:26:53.425978Z","shell.execute_reply":"2023-10-18T18:26:53.425375Z"},"papermill":{"duration":1.203607,"end_time":"2023-10-18T18:26:53.436035","exception":false,"start_time":"2023-10-18T18:26:52.232428","status":"completed"},"tags":[],"id":"abc2b7ad"},"outputs":[],"source":["plt.figure(1, figsize=(15, 9))\n","plt.axis('off')\n","n = 0\n","for i in range(4):\n","\n","    n += 1\n","    random_img = random.choice(image_list_text)\n","    imgs = imread(random_img)\n","    plt.suptitle(\"Random images of people who text with their phone\",\n","                 fontdict = font, fontsize=25\n","                 ,backgroundcolor=background_color)\n","    plt.subplot(2,2,n)\n","    plt.imshow(imgs)\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"c19ef073","metadata":{"execution":{"iopub.execute_input":"2023-10-18T18:26:53.599774Z","iopub.status.busy":"2023-10-18T18:26:53.598756Z","iopub.status.idle":"2023-10-18T18:26:54.536813Z","shell.execute_reply":"2023-10-18T18:26:54.536214Z"},"papermill":{"duration":1.034098,"end_time":"2023-10-18T18:26:54.549203","exception":false,"start_time":"2023-10-18T18:26:53.515105","status":"completed"},"tags":[],"id":"c19ef073"},"outputs":[],"source":["plt.figure(1, figsize=(15, 9))\n","plt.axis('off')\n","n = 0\n","for i in range(4):\n","\n","    n += 1\n","    random_img = random.choice(image_list_turn)\n","    imgs = imread(random_img)\n","    plt.suptitle(\"Random images of people who turn around\",\n","                 fontdict = font, fontsize=25\n","                 ,backgroundcolor=background_color)\n","    plt.subplot(2,2,n)\n","    plt.imshow(imgs)\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"649221cf","metadata":{"execution":{"iopub.execute_input":"2023-10-18T18:26:54.755124Z","iopub.status.busy":"2023-10-18T18:26:54.754122Z","iopub.status.idle":"2023-10-18T18:26:55.784034Z","shell.execute_reply":"2023-10-18T18:26:55.783403Z"},"papermill":{"duration":1.131773,"end_time":"2023-10-18T18:26:55.785990","exception":false,"start_time":"2023-10-18T18:26:54.654217","status":"completed"},"tags":[],"id":"649221cf"},"outputs":[],"source":["plt.figure(1, figsize=(15, 9))\n","plt.axis('off')\n","n = 0\n","for i in range(4):\n","\n","    n += 1\n","    random_img = random.choice(image_list_safe)\n","    imgs = imread(random_img)\n","    plt.suptitle(\"Random images of people who drive safely\",\n","                 fontdict = font, fontsize=25\n","                 ,backgroundcolor=background_color)\n","    plt.subplot(2,2,n)\n","    plt.imshow(imgs)\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"12a69f7e","metadata":{"execution":{"iopub.execute_input":"2023-10-18T18:26:56.039513Z","iopub.status.busy":"2023-10-18T18:26:56.038997Z","iopub.status.idle":"2023-10-18T18:26:56.998436Z","shell.execute_reply":"2023-10-18T18:26:56.997769Z"},"papermill":{"duration":1.101158,"end_time":"2023-10-18T18:26:57.010022","exception":false,"start_time":"2023-10-18T18:26:55.908864","status":"completed"},"tags":[],"id":"12a69f7e"},"outputs":[],"source":["plt.figure(1, figsize=(15, 9))\n","plt.axis('off')\n","n = 0\n","for i in range(4):\n","\n","    n += 1\n","    random_img = random.choice(image_list_other)\n","    imgs = imread(random_img)\n","    plt.suptitle(\"Random images of people who are in other positions\",\n","                 fontdict = font, fontsize=25\n","                 ,backgroundcolor=background_color)\n","    plt.subplot(2,2,n)\n","    plt.imshow(imgs)\n","\n","plt.show()"]},{"cell_type":"markdown","id":"96880b5a","metadata":{"papermill":{"duration":0.136596,"end_time":"2023-10-18T18:26:57.298768","exception":false,"start_time":"2023-10-18T18:26:57.162172","status":"completed"},"tags":[],"id":"96880b5a"},"source":["<div style = 'border : 3px solid non; background-color:#f2f2f2 ; ;padding:10px'>\n","\n","\n","* **3. Split data to train, test, prediction**\n","\n","  - I deciede to devide dataset in this situation: 1- train = 75%, 2- test = 15% and 3- valid = 5%\n","   \n","   \n"]},{"cell_type":"code","execution_count":null,"id":"cc2cc713","metadata":{"execution":{"iopub.execute_input":"2023-10-18T18:26:57.578599Z","iopub.status.busy":"2023-10-18T18:26:57.578273Z","iopub.status.idle":"2023-10-18T18:26:57.582980Z","shell.execute_reply":"2023-10-18T18:26:57.582369Z"},"papermill":{"duration":0.152465,"end_time":"2023-10-18T18:26:57.584813","exception":false,"start_time":"2023-10-18T18:26:57.432348","status":"completed"},"tags":[],"id":"cc2cc713"},"outputs":[],"source":["print(\"Number of samples in (Class = Other) = \" ,len(image_list_other))\n","print(\"Number of samples in (Class = Safe Driving) = \" ,len(image_list_safe))\n","print(\"Number of samples in (Class = Talking Phone) = \" ,len(image_list_talking))\n","print(\"Number of samples in (Class = Texting Phone) = \" ,len(image_list_text))\n","print(\"Number of samples in (Class = Turning) = \" ,len(image_list_turn))"]},{"cell_type":"code","execution_count":null,"id":"4cc86a25","metadata":{"execution":{"iopub.execute_input":"2023-10-18T18:26:57.872960Z","iopub.status.busy":"2023-10-18T18:26:57.872615Z","iopub.status.idle":"2023-10-18T18:26:57.877617Z","shell.execute_reply":"2023-10-18T18:26:57.877092Z"},"papermill":{"duration":0.153725,"end_time":"2023-10-18T18:26:57.879306","exception":false,"start_time":"2023-10-18T18:26:57.725581","status":"completed"},"tags":[],"id":"4cc86a25"},"outputs":[],"source":["print(.75*len(image_list_other) , .2*len(image_list_other) ,.05*len(image_list_other))\n","print(.75*len(image_list_safe) , .2*len(image_list_safe) ,.05*len(image_list_safe))\n","print(.75*len(image_list_talking) , .2*len(image_list_talking) ,.05*len(image_list_talking))\n","print(.75*len(image_list_text) , .2*len(image_list_text) ,.05*len(image_list_text))\n","print(.75*len(image_list_turn) , .2*len(image_list_turn) ,.05*len(image_list_turn))"]},{"cell_type":"code","execution_count":null,"id":"b1473987","metadata":{"execution":{"iopub.execute_input":"2023-10-18T18:26:58.164366Z","iopub.status.busy":"2023-10-18T18:26:58.163482Z","iopub.status.idle":"2023-10-18T18:26:58.171616Z","shell.execute_reply":"2023-10-18T18:26:58.171009Z"},"papermill":{"duration":0.154203,"end_time":"2023-10-18T18:26:58.173424","exception":false,"start_time":"2023-10-18T18:26:58.019221","status":"completed"},"tags":[],"id":"b1473987"},"outputs":[],"source":["print(\"Train\",\"Test\", \"Valid\")\n","\n","train_other = image_list_other[:1589]\n","test_other = image_list_other[1589:2012]\n","valid_other = image_list_other[2012:]\n","\n","print (len(train_other), len(test_other), len(valid_other))\n","\n","train_safe = image_list_safe[:1652]\n","test_safe = image_list_safe[1652:2092]\n","valid_safe = image_list_safe[2092:]\n","\n","print (len(train_safe), len(test_safe), len(valid_safe))\n","\n","train_talking = image_list_talking[:1626]\n","test_talking = image_list_talking[1626:2059]\n","valid_talking = image_list_talking[2059:]\n","\n","print (len(train_talking), len(test_talking), len(valid_talking))\n","\n","train_text = image_list_text[:1652]\n","test_text = image_list_text[1652:2092]\n","valid_text = image_list_text[2092:]\n","\n","print (len(train_text), len(test_text), len(valid_text))\n","\n","train_turn = image_list_turn[:1547]\n","test_turn = image_list_turn[1547:1959]\n","valid_turn = image_list_turn[1959:]\n","\n","print (len(train_turn), len(test_turn), len(valid_turn))"]},{"cell_type":"markdown","id":"0bce4301","metadata":{"papermill":{"duration":0.155597,"end_time":"2023-10-18T18:26:58.470655","exception":false,"start_time":"2023-10-18T18:26:58.315058","status":"completed"},"tags":[],"id":"0bce4301"},"source":["<div style = 'border : 3px solid non; background-color:#f2f2f2 ; ;padding:10px'>\n","\n","\n","* **4. Create dataframes for train, test, prediction**\n","\n","  - If you don't want to use directly from the directories and you want to create some lists of images, you can turn the format of data to dataframes and use **flow from dataframe** method that we see in the next cells.\n","    For this approach, create a **label** column for use it as a tag or label\n","   \n"]},{"cell_type":"code","execution_count":null,"id":"508887cb","metadata":{"execution":{"iopub.execute_input":"2023-10-18T18:26:58.765514Z","iopub.status.busy":"2023-10-18T18:26:58.764492Z","iopub.status.idle":"2023-10-18T18:26:58.773145Z","shell.execute_reply":"2023-10-18T18:26:58.772462Z"},"papermill":{"duration":0.155774,"end_time":"2023-10-18T18:26:58.774628","exception":false,"start_time":"2023-10-18T18:26:58.618854","status":"completed"},"tags":[],"id":"508887cb"},"outputs":[],"source":["train_other_df = pd.DataFrame({'image':train_other, 'label':'Other'})\n","train_safe_df = pd.DataFrame({'image':train_safe, 'label':'Safe'})\n","train_talking_df = pd.DataFrame({'image':train_talking, 'label':'Talk'})\n","train_text_df = pd.DataFrame({'image':train_text, 'label':'Text'})\n","train_turn_df = pd.DataFrame({'image':train_turn, 'label':'Turn'})"]},{"cell_type":"code","execution_count":null,"id":"d03ed3ed","metadata":{"execution":{"iopub.execute_input":"2023-10-18T18:26:59.071705Z","iopub.status.busy":"2023-10-18T18:26:59.071386Z","iopub.status.idle":"2023-10-18T18:26:59.077094Z","shell.execute_reply":"2023-10-18T18:26:59.076349Z"},"papermill":{"duration":0.159678,"end_time":"2023-10-18T18:26:59.078544","exception":false,"start_time":"2023-10-18T18:26:58.918866","status":"completed"},"tags":[],"id":"d03ed3ed"},"outputs":[],"source":["test_other_df = pd.DataFrame({'image':test_other, 'label':'Other'})\n","test_safe_df = pd.DataFrame({'image':test_safe, 'label':'Safe'})\n","test_talking_df = pd.DataFrame({'image':test_talking, 'label':'Talk'})\n","test_text_df = pd.DataFrame({'image':test_text, 'label':'Text'})\n","test_turn_df = pd.DataFrame({'image':test_turn, 'label':'Turn'})"]},{"cell_type":"code","execution_count":null,"id":"142d3ffa","metadata":{"execution":{"iopub.execute_input":"2023-10-18T18:26:59.376851Z","iopub.status.busy":"2023-10-18T18:26:59.375764Z","iopub.status.idle":"2023-10-18T18:26:59.382584Z","shell.execute_reply":"2023-10-18T18:26:59.381763Z"},"papermill":{"duration":0.159903,"end_time":"2023-10-18T18:26:59.384388","exception":false,"start_time":"2023-10-18T18:26:59.224485","status":"completed"},"tags":[],"id":"142d3ffa"},"outputs":[],"source":["valid_other_df = pd.DataFrame({'image':valid_other, 'label':'Other'})\n","valid_safe_df = pd.DataFrame({'image':valid_safe, 'label':'Safe'})\n","valid_talking_df = pd.DataFrame({'image':valid_talking, 'label':'Talk'})\n","valid_text_df = pd.DataFrame({'image':valid_text, 'label':'Text'})\n","valid_turn_df = pd.DataFrame({'image':valid_turn, 'label':'Turn'})"]},{"cell_type":"code","execution_count":null,"id":"10c20f31","metadata":{"execution":{"iopub.execute_input":"2023-10-18T18:26:59.679519Z","iopub.status.busy":"2023-10-18T18:26:59.678445Z","iopub.status.idle":"2023-10-18T18:26:59.687905Z","shell.execute_reply":"2023-10-18T18:26:59.687184Z"},"papermill":{"duration":0.161213,"end_time":"2023-10-18T18:26:59.689604","exception":false,"start_time":"2023-10-18T18:26:59.528391","status":"completed"},"tags":[],"id":"10c20f31"},"outputs":[],"source":["train_df = pd.concat([train_other_df, train_safe_df, train_talking_df, train_text_df, train_turn_df])\n","test_df = pd.concat([test_other_df, test_safe_df, test_talking_df, test_text_df, test_turn_df])\n","val_df = pd.concat([valid_other_df, valid_safe_df, valid_talking_df, valid_text_df, valid_turn_df])"]},{"cell_type":"code","execution_count":null,"id":"9e309ad4","metadata":{"execution":{"iopub.execute_input":"2023-10-18T18:26:59.988742Z","iopub.status.busy":"2023-10-18T18:26:59.987308Z","iopub.status.idle":"2023-10-18T18:27:00.007596Z","shell.execute_reply":"2023-10-18T18:27:00.006687Z"},"papermill":{"duration":0.172716,"end_time":"2023-10-18T18:27:00.009242","exception":false,"start_time":"2023-10-18T18:26:59.836526","status":"completed"},"tags":[],"id":"9e309ad4"},"outputs":[],"source":["train_df.head()"]},{"cell_type":"code","execution_count":null,"id":"f4fe9280","metadata":{"execution":{"iopub.execute_input":"2023-10-18T18:27:00.306239Z","iopub.status.busy":"2023-10-18T18:27:00.305944Z","iopub.status.idle":"2023-10-18T18:27:00.310061Z","shell.execute_reply":"2023-10-18T18:27:00.309469Z"},"papermill":{"duration":0.156165,"end_time":"2023-10-18T18:27:00.311646","exception":false,"start_time":"2023-10-18T18:27:00.155481","status":"completed"},"tags":[],"id":"f4fe9280"},"outputs":[],"source":["print(\"Number of rows in train dataframe is: \", len(train_df))\n","print(\"Number of rows in test dataframe is: \", len(test_df))\n","print(\"Number of rows in val dataframe is: \", len(val_df))"]},{"cell_type":"markdown","id":"c3347041","metadata":{"papermill":{"duration":0.141821,"end_time":"2023-10-18T18:27:00.596344","exception":false,"start_time":"2023-10-18T18:27:00.454523","status":"completed"},"tags":[],"id":"c3347041"},"source":["<div style = 'border : 3px solid non; background-color:#f2f2f2 ; ;padding:10px'>\n","\n","\n","* **5. Calculate the ratio of images**\n","\n","  - It's really important in some cases for knowing the ratio of images that we use. We can use **cv2.imread** for this part.\n","   \n"]},{"cell_type":"code","execution_count":null,"id":"641d79d0","metadata":{"execution":{"iopub.execute_input":"2023-10-18T18:27:00.885038Z","iopub.status.busy":"2023-10-18T18:27:00.883966Z","iopub.status.idle":"2023-10-18T18:27:00.887996Z","shell.execute_reply":"2023-10-18T18:27:00.887264Z"},"papermill":{"duration":0.152302,"end_time":"2023-10-18T18:27:00.889594","exception":false,"start_time":"2023-10-18T18:27:00.737292","status":"completed"},"tags":[],"id":"641d79d0"},"outputs":[],"source":["random_img_height = random.choice(train_other)"]},{"cell_type":"code","execution_count":null,"id":"936e69ec","metadata":{"execution":{"iopub.execute_input":"2023-10-18T18:27:01.178061Z","iopub.status.busy":"2023-10-18T18:27:01.177300Z","iopub.status.idle":"2023-10-18T18:27:01.234901Z","shell.execute_reply":"2023-10-18T18:27:01.233948Z"},"papermill":{"duration":0.204556,"end_time":"2023-10-18T18:27:01.236394","exception":false,"start_time":"2023-10-18T18:27:01.031838","status":"completed"},"tags":[],"id":"936e69ec"},"outputs":[],"source":["image= cv2.imread(random_img_height)\n","\n","height, width= image.shape[:2]\n","\n","print(\"The height is \", height)\n","\n","print(\"The width is \", width)"]},{"cell_type":"markdown","id":"d53cbee1","metadata":{"papermill":{"duration":0.145542,"end_time":"2023-10-18T18:27:01.526727","exception":false,"start_time":"2023-10-18T18:27:01.381185","status":"completed"},"tags":[],"id":"d53cbee1"},"source":["<div style = 'border : 3px solid non; background-color:#f2f2f2 ; ;padding:10px'>\n","\n","\n","* **6. Define hyperparameters**\n","\n","  - Some of the hyperparameters should be considering before the model goes to start. I set Batch size as 64, ofcourse you can use 32 or any other popular numbers or you can use some functions for checking the best number of that but it costs lots of run time and I just want to pick some experimental value. Also for AlexNet, ResNet and VGGNet its popular to use height and width as a 240*240 format.\n","   \n"]},{"cell_type":"code","execution_count":null,"id":"58684907","metadata":{"execution":{"iopub.execute_input":"2023-10-18T18:27:01.822282Z","iopub.status.busy":"2023-10-18T18:27:01.821978Z","iopub.status.idle":"2023-10-18T18:27:01.825738Z","shell.execute_reply":"2023-10-18T18:27:01.824977Z"},"papermill":{"duration":0.153362,"end_time":"2023-10-18T18:27:01.827304","exception":false,"start_time":"2023-10-18T18:27:01.673942","status":"completed"},"tags":[],"id":"58684907"},"outputs":[],"source":["Batch_size = 64\n","Img_height = 240\n","Img_width = 240"]},{"cell_type":"markdown","id":"7fd62cfc","metadata":{"papermill":{"duration":0.146068,"end_time":"2023-10-18T18:27:02.121228","exception":false,"start_time":"2023-10-18T18:27:01.975160","status":"completed"},"tags":[],"id":"7fd62cfc"},"source":["<div style = 'border : 3px solid non; background-color:#f2f2f2 ; ;padding:10px'>\n","\n","\n","* **7. Rescale the images**\n","\n","  - Its really important to rescale the images and put all of the images in a same shape for input layer of model.\n","   \n"]},{"cell_type":"code","execution_count":null,"id":"1bf9b6d5","metadata":{"execution":{"iopub.execute_input":"2023-10-18T18:27:02.430423Z","iopub.status.busy":"2023-10-18T18:27:02.429380Z","iopub.status.idle":"2023-10-18T18:27:02.433910Z","shell.execute_reply":"2023-10-18T18:27:02.433163Z"},"papermill":{"duration":0.153291,"end_time":"2023-10-18T18:27:02.435386","exception":false,"start_time":"2023-10-18T18:27:02.282095","status":"completed"},"tags":[],"id":"1bf9b6d5"},"outputs":[],"source":["trainGenerator = ImageDataGenerator(rescale=1./255.)\n","valGenerator = ImageDataGenerator(rescale=1./255.)\n","testGenerator = ImageDataGenerator(rescale=1./255.)"]},{"cell_type":"markdown","id":"173264a8","metadata":{"papermill":{"duration":0.1435,"end_time":"2023-10-18T18:27:02.725360","exception":false,"start_time":"2023-10-18T18:27:02.581860","status":"completed"},"tags":[],"id":"173264a8"},"source":["<div style = 'border : 3px solid non; background-color:#f2f2f2 ; ;padding:10px'>\n","\n","\n","* **8. Input Model**\n","\n","  - Now we can use **flow from dataframe** that mentioned before. It helps to call images from some dataframes and mention the label as target.\n","   \n"]},{"cell_type":"code","execution_count":null,"id":"acd581cd","metadata":{"execution":{"iopub.execute_input":"2023-10-18T18:27:03.020516Z","iopub.status.busy":"2023-10-18T18:27:03.020219Z","iopub.status.idle":"2023-10-18T18:27:28.161804Z","shell.execute_reply":"2023-10-18T18:27:28.161110Z"},"papermill":{"duration":25.29124,"end_time":"2023-10-18T18:27:28.163486","exception":false,"start_time":"2023-10-18T18:27:02.872246","status":"completed"},"tags":[],"id":"acd581cd"},"outputs":[],"source":["trainDataset = trainGenerator.flow_from_dataframe(\n","  dataframe=train_df,\n","  class_mode=\"categorical\",\n","  x_col=\"image\",\n","  y_col=\"label\",\n","  batch_size=Batch_size,\n","  seed=42,\n","  shuffle=True,\n","  target_size=(Img_height,Img_width) #set the height and width of the images\n",")\n","\n","testDataset = testGenerator.flow_from_dataframe(\n","  dataframe=test_df,\n","  class_mode='categorical',\n","  x_col=\"image\",\n","  y_col=\"label\",\n","  batch_size=Batch_size,\n","  seed=42,\n","  shuffle=True,\n","  target_size=(Img_height,Img_width)\n",")\n","\n","valDataset = valGenerator.flow_from_dataframe(\n","  dataframe=val_df,\n","  class_mode='categorical',\n","  x_col=\"image\",\n","  y_col=\"label\",\n","  batch_size=Batch_size,\n","  seed=42,\n","  shuffle=True,\n","  target_size=(Img_height,Img_width)\n",")"]},{"cell_type":"markdown","id":"a992e343","metadata":{"papermill":{"duration":0.142107,"end_time":"2023-10-18T18:27:28.451318","exception":false,"start_time":"2023-10-18T18:27:28.309211","status":"completed"},"tags":[],"id":"a992e343"},"source":["<a id=\"6\"></a>\n","# <p style=\"padding:10px;background-color:#fab72f ;margin:0;color:#ffffff;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:500\">AlexNet </p>"]},{"cell_type":"markdown","id":"8ebfacd8","metadata":{"papermill":{"duration":0.141323,"end_time":"2023-10-18T18:27:28.747502","exception":false,"start_time":"2023-10-18T18:27:28.606179","status":"completed"},"tags":[],"id":"8ebfacd8"},"source":["<div style = 'border : 3px solid non; background-color:#f2f2f2 ; ;padding:10px'>\n","\n","\n","* **1. What is AlexNet?**\n","\n","  - AlexNet is a convolutional neural network (CNN) architecture that was introduced by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton in 2012. It is used primarily for image recognition and classification tasks. AlexNet was the winner of the ImageNet Large Scale Visual Recognition Challenge in 2012, which marked a breakthrough in deep learning. The network contains eight layers; the first five are convolutional layers, some of them followed by max-pooling layers, and the last three are fully connected layers. The network is split into two copies, each run on one GPU. AlexNet uses ReLU activation functions and dropout regularization to prevent overfitting. The architecture of AlexNet has inspired many other CNN architectures that have been developed since its introduction.\n","   \n"]},{"cell_type":"markdown","id":"19853cf3","metadata":{"papermill":{"duration":0.141462,"end_time":"2023-10-18T18:27:29.032882","exception":false,"start_time":"2023-10-18T18:27:28.891420","status":"completed"},"tags":[],"id":"19853cf3"},"source":["<div style = 'border : 3px solid non; background-color:#f2f2f2 ; ;padding:10px'>\n","\n","\n","* **2. Model Structure**\n","\n","  - You can see the structure of AlexNet in the cell below.\n","   \n"]},{"cell_type":"code","execution_count":null,"id":"b8ab4786","metadata":{"execution":{"iopub.execute_input":"2023-10-18T18:27:29.327985Z","iopub.status.busy":"2023-10-18T18:27:29.327123Z","iopub.status.idle":"2023-10-18T18:27:35.695844Z","shell.execute_reply":"2023-10-18T18:27:35.695325Z"},"papermill":{"duration":6.529827,"end_time":"2023-10-18T18:27:35.710111","exception":false,"start_time":"2023-10-18T18:27:29.180284","status":"completed"},"tags":[],"id":"b8ab4786"},"outputs":[],"source":["def AlexNet():\n","    inp = layers.Input((240, 240, 3))\n","    x = layers.Conv2D(96, 11, 4, activation='relu')(inp)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.MaxPooling2D(3, 2)(x)\n","    x = layers.Conv2D(256, 5, 1, activation='relu')(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.MaxPooling2D(3, 2)(x)\n","    x = layers.Conv2D(384, 3, 1, activation='relu')(x)\n","    x = layers.Conv2D(384, 3, 1, activation='relu')(x)\n","    x = layers.Conv2D(256, 3, 1, activation='relu')(x)\n","    x = layers.MaxPooling2D(3, 2)(x)\n","    x = layers.Flatten()(x)\n","    x = layers.Dense(4096, activation='relu')(x)\n","    x = layers.Dropout(0.5)(x)\n","    x = layers.Dense(4096, activation='relu')(x)\n","    x = layers.Dropout(0.5)(x)\n","    x = layers.Dense(5, activation='softmax')(x)\n","\n","    model_Alex = models.Model(inputs=inp, outputs=x)\n","\n","    return model_Alex\n","\n","model_Alex = AlexNet()\n","model_Alex.summary()"]},{"cell_type":"code","execution_count":null,"id":"6d599096","metadata":{"execution":{"iopub.execute_input":"2023-10-18T18:27:35.992398Z","iopub.status.busy":"2023-10-18T18:27:35.992113Z","iopub.status.idle":"2023-10-18T18:27:36.483100Z","shell.execute_reply":"2023-10-18T18:27:36.482524Z"},"papermill":{"duration":0.633219,"end_time":"2023-10-18T18:27:36.485137","exception":false,"start_time":"2023-10-18T18:27:35.851918","status":"completed"},"tags":[],"id":"6d599096"},"outputs":[],"source":["tf.keras.utils.plot_model(\n","    model_Alex,\n","    to_file='alex_model.png',\n","    show_shapes=True,\n","    show_dtype=False,\n","    show_layer_names=True,\n","    show_layer_activations=True,\n","    dpi=100\n",")"]},{"cell_type":"code","execution_count":null,"id":"d7204e60","metadata":{"execution":{"iopub.execute_input":"2023-10-18T18:27:36.781050Z","iopub.status.busy":"2023-10-18T18:27:36.780191Z","iopub.status.idle":"2023-10-18T18:27:36.796340Z","shell.execute_reply":"2023-10-18T18:27:36.795753Z"},"papermill":{"duration":0.162154,"end_time":"2023-10-18T18:27:36.798029","exception":false,"start_time":"2023-10-18T18:27:36.635875","status":"completed"},"tags":[],"id":"d7204e60"},"outputs":[],"source":["model_Alex.compile(loss=BinaryCrossentropy(),\n","              optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"id":"9e181075","metadata":{"execution":{"iopub.execute_input":"2023-10-18T18:27:37.091036Z","iopub.status.busy":"2023-10-18T18:27:37.090174Z","iopub.status.idle":"2023-10-18T18:47:08.987633Z","shell.execute_reply":"2023-10-18T18:47:08.986479Z"},"papermill":{"duration":1172.047227,"end_time":"2023-10-18T18:47:08.989608","exception":false,"start_time":"2023-10-18T18:27:36.942381","status":"completed"},"tags":[],"id":"9e181075"},"outputs":[],"source":["Alex_model = model_Alex.fit(trainDataset, epochs=20, validation_data=valDataset)"]},{"cell_type":"markdown","id":"d7728891","metadata":{"papermill":{"duration":0.271445,"end_time":"2023-10-18T18:47:09.537040","exception":false,"start_time":"2023-10-18T18:47:09.265595","status":"completed"},"tags":[],"id":"d7728891"},"source":["<div style = 'border : 3px solid non; background-color:#f2f2f2 ; ;padding:10px'>\n","\n","\n","* **3. Output**\n","\n","  - We can plot the output of model for each epoch. For this matter, we can use **.history** and extract the **train and valid loss and accuracy**. Then we can plot both of them and find out the path of learning.\n","   \n"]},{"cell_type":"code","execution_count":null,"id":"b42f9c3f","metadata":{"execution":{"iopub.execute_input":"2023-10-18T18:47:10.097573Z","iopub.status.busy":"2023-10-18T18:47:10.096961Z","iopub.status.idle":"2023-10-18T18:47:10.100808Z","shell.execute_reply":"2023-10-18T18:47:10.100140Z"},"papermill":{"duration":0.275621,"end_time":"2023-10-18T18:47:10.102293","exception":false,"start_time":"2023-10-18T18:47:09.826672","status":"completed"},"tags":[],"id":"b42f9c3f"},"outputs":[],"source":["training_loss_alex = Alex_model.history['loss']\n","val_loss_alex = Alex_model.history['val_loss']\n","training_acc_alex = Alex_model.history['accuracy']\n","val_acc_alex = Alex_model.history['val_accuracy']"]},{"cell_type":"code","execution_count":null,"id":"57eea0bf","metadata":{"execution":{"iopub.execute_input":"2023-10-18T18:47:10.628796Z","iopub.status.busy":"2023-10-18T18:47:10.627911Z","iopub.status.idle":"2023-10-18T18:47:10.959211Z","shell.execute_reply":"2023-10-18T18:47:10.958536Z"},"papermill":{"duration":0.598209,"end_time":"2023-10-18T18:47:10.960682","exception":false,"start_time":"2023-10-18T18:47:10.362473","status":"completed"},"tags":[],"id":"57eea0bf"},"outputs":[],"source":["epoch_count = range(1, len(training_loss_alex) + 1)\n","\n","# Visualize loss history\n","plt.figure(figsize=(10,5), dpi=200)\n","plt.plot(epoch_count, training_loss_alex, 'r--', color= 'navy')\n","plt.plot(epoch_count, val_loss_alex, '--bo',color= 'orangered', linewidth = '2.5', label='line with marker')\n","plt.legend(['Training Loss', 'Val Loss'])\n","plt.title('Number of epochs & Loss in ALEXNET')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.xticks(np.arange(1,21,1))\n","plt.show();"]},{"cell_type":"code","execution_count":null,"id":"f481e712","metadata":{"execution":{"iopub.execute_input":"2023-10-18T18:47:11.509807Z","iopub.status.busy":"2023-10-18T18:47:11.509465Z","iopub.status.idle":"2023-10-18T18:47:11.821227Z","shell.execute_reply":"2023-10-18T18:47:11.820567Z"},"papermill":{"duration":0.592652,"end_time":"2023-10-18T18:47:11.822865","exception":false,"start_time":"2023-10-18T18:47:11.230213","status":"completed"},"tags":[],"id":"f481e712"},"outputs":[],"source":["plt.figure(figsize=(10,5), dpi=200)\n","plt.plot(epoch_count, training_acc_alex, 'r--', color= 'navy')\n","plt.plot(epoch_count, val_acc_alex, '--bo',color= 'orangered', linewidth = '2.5', label='line with marker')\n","plt.legend(['Training Acc', 'Val Acc'])\n","plt.title('Number of epochs & Accuracy in ALEXNET')\n","plt.xlabel('Epoch')\n","plt.ylabel('Acc')\n","plt.xticks(np.arange(1,21,1))\n","plt.plot();\n","plt.show();"]},{"cell_type":"markdown","id":"d20a4b37","metadata":{"papermill":{"duration":0.28532,"end_time":"2023-10-18T18:47:12.433896","exception":false,"start_time":"2023-10-18T18:47:12.148576","status":"completed"},"tags":[],"id":"d20a4b37"},"source":["<a id=\"7\"></a>\n","# <p style=\"padding:10px;background-color:#fab72f ;margin:0;color:#ffffff;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:500\">VGGNet </p>"]},{"cell_type":"markdown","id":"445ea6ec","metadata":{"papermill":{"duration":0.263859,"end_time":"2023-10-18T18:47:12.968325","exception":false,"start_time":"2023-10-18T18:47:12.704466","status":"completed"},"tags":[],"id":"445ea6ec"},"source":["<div style = 'border : 3px solid non; background-color:#f2f2f2 ; ;padding:10px'>\n","\n","\n","* **1. What is VGGNet?**\n","\n","  - VGGNet, also known as Visual Geometry Group, is a standard deep Convolutional Neural Network (CNN) architecture with multiple layers. The ‚Äúdeep‚Äù refers to the number of layers with VGG-16 or VGG-19 consisting of 16 and 19 convolutional layers, respectively. The VGG architecture is the basis of ground-breaking object recognition models. Developed as a deep neural network, the VGGNet also surpasses baselines on many tasks and datasets beyond ImageNet. Moreover, it is still one of the most popular image recognition architectures.\n","   \n"]},{"cell_type":"markdown","id":"3a60223e","metadata":{"papermill":{"duration":0.266129,"end_time":"2023-10-18T18:47:13.515494","exception":false,"start_time":"2023-10-18T18:47:13.249365","status":"completed"},"tags":[],"id":"3a60223e"},"source":["<div style = 'border : 3px solid non; background-color:#f2f2f2 ; ;padding:10px'>\n","\n","\n","* **2. Model Structure**\n","\n","  - You can see the structure of VGGNet in the cell below.\n","   \n"]},{"cell_type":"code","execution_count":null,"id":"310302af","metadata":{"execution":{"iopub.execute_input":"2023-10-18T18:47:14.058385Z","iopub.status.busy":"2023-10-18T18:47:14.057050Z","iopub.status.idle":"2023-10-18T18:47:14.286153Z","shell.execute_reply":"2023-10-18T18:47:14.285627Z"},"papermill":{"duration":0.514268,"end_time":"2023-10-18T18:47:14.300210","exception":false,"start_time":"2023-10-18T18:47:13.785942","status":"completed"},"tags":[],"id":"310302af"},"outputs":[],"source":["def VGGNet():\n","    inp = layers.Input((240, 240, 3))\n","    x = layers.Conv2D(64, 3, 1, activation='relu')(inp)\n","    x = layers.Conv2D(64, 3, 1, activation='relu')(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.MaxPooling2D(2, 2)(x)\n","    x = layers.Conv2D(128, 3, 1, activation='relu')(x)\n","    x = layers.Conv2D(128, 3, 1, activation='relu')(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.MaxPooling2D(2, 2)(x)\n","    x = layers.Conv2D(256, 3, 1, activation='relu')(x)\n","    x = layers.Conv2D(256, 3, 1, activation='relu')(x)\n","    x = layers.Conv2D(256, 3, 1, activation='relu')(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.MaxPooling2D(2, 2)(x)\n","    x = layers.Conv2D(512, 3, 1, activation='relu')(x)\n","    x = layers.Conv2D(512, 3, 1, activation='relu')(x)\n","    x = layers.Conv2D(512, 3, 1, activation='relu')(x)\n","    x = layers.MaxPooling2D(2, 2)(x)\n","    x = layers.Flatten()(x)\n","    x = layers.Dense(4096, activation='relu')(x)\n","    x = layers.Dropout(0.5)(x)\n","    x = layers.Dense(4096, activation='relu')(x)\n","    x = layers.Dropout(0.5)(x)\n","    x = layers.Dense(5, activation='softmax')(x)\n","\n","    model_VGG = models.Model(inputs=inp, outputs=x)\n","\n","    return model_VGG\n","\n","model_VGG = VGGNet()\n","model_VGG.summary()"]},{"cell_type":"code","execution_count":null,"id":"7941e076","metadata":{"execution":{"iopub.execute_input":"2023-10-18T18:47:14.842465Z","iopub.status.busy":"2023-10-18T18:47:14.842080Z","iopub.status.idle":"2023-10-18T18:47:15.028314Z","shell.execute_reply":"2023-10-18T18:47:15.027715Z"},"papermill":{"duration":0.466047,"end_time":"2023-10-18T18:47:15.032146","exception":false,"start_time":"2023-10-18T18:47:14.566099","status":"completed"},"tags":[],"id":"7941e076"},"outputs":[],"source":["tf.keras.utils.plot_model(\n","    model_VGG,\n","    to_file='vgg_model.png',\n","    show_shapes=True,\n","    show_dtype=False,\n","    show_layer_names=True,\n","    show_layer_activations=True,\n","    dpi=100\n",")"]},{"cell_type":"code","execution_count":null,"id":"87f692d1","metadata":{"execution":{"iopub.execute_input":"2023-10-18T18:47:15.604864Z","iopub.status.busy":"2023-10-18T18:47:15.604311Z","iopub.status.idle":"2023-10-18T18:47:15.616376Z","shell.execute_reply":"2023-10-18T18:47:15.615725Z"},"papermill":{"duration":0.294825,"end_time":"2023-10-18T18:47:15.618128","exception":false,"start_time":"2023-10-18T18:47:15.323303","status":"completed"},"tags":[],"id":"87f692d1"},"outputs":[],"source":["model_VGG.compile(loss=BinaryCrossentropy(),\n","              optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"id":"cf43d1b0","metadata":{"execution":{"iopub.execute_input":"2023-10-18T18:47:16.233378Z","iopub.status.busy":"2023-10-18T18:47:16.232830Z","iopub.status.idle":"2023-10-18T19:30:03.595384Z","shell.execute_reply":"2023-10-18T19:30:03.594446Z"},"papermill":{"duration":2567.679341,"end_time":"2023-10-18T19:30:03.598006","exception":false,"start_time":"2023-10-18T18:47:15.918665","status":"completed"},"tags":[],"id":"cf43d1b0"},"outputs":[],"source":["VGG_model = model_VGG.fit(trainDataset, epochs=20, validation_data=valDataset)"]},{"cell_type":"markdown","id":"dc6a3e5c","metadata":{"papermill":{"duration":0.446779,"end_time":"2023-10-18T19:30:04.450015","exception":false,"start_time":"2023-10-18T19:30:04.003236","status":"completed"},"tags":[],"id":"dc6a3e5c"},"source":["<div style = 'border : 3px solid non; background-color:#f2f2f2 ; ;padding:10px'>\n","\n","\n","* **3. Output**\n","\n","  - We can plot the output of model for each epoch. For this matter, we can use **.history** and extract the **train and valid loss and accuracy**. Then we can plot both of them and find out the path of learning.\n","   \n"]},{"cell_type":"code","execution_count":null,"id":"bd85152f","metadata":{"execution":{"iopub.execute_input":"2023-10-18T19:30:05.241084Z","iopub.status.busy":"2023-10-18T19:30:05.240793Z","iopub.status.idle":"2023-10-18T19:30:05.245248Z","shell.execute_reply":"2023-10-18T19:30:05.244272Z"},"papermill":{"duration":0.400701,"end_time":"2023-10-18T19:30:05.246732","exception":false,"start_time":"2023-10-18T19:30:04.846031","status":"completed"},"tags":[],"id":"bd85152f"},"outputs":[],"source":["training_loss_vgg = VGG_model.history['loss']\n","val_loss_vgg = VGG_model.history['val_loss']\n","training_acc_vgg = VGG_model.history['accuracy']\n","val_acc_vgg = VGG_model.history['val_accuracy']"]},{"cell_type":"code","execution_count":null,"id":"d1cb5d44","metadata":{"execution":{"iopub.execute_input":"2023-10-18T19:30:06.050217Z","iopub.status.busy":"2023-10-18T19:30:06.049207Z","iopub.status.idle":"2023-10-18T19:30:06.371068Z","shell.execute_reply":"2023-10-18T19:30:06.370496Z"},"papermill":{"duration":0.723335,"end_time":"2023-10-18T19:30:06.372541","exception":false,"start_time":"2023-10-18T19:30:05.649206","status":"completed"},"tags":[],"id":"d1cb5d44"},"outputs":[],"source":["plt.figure(figsize=(10,5), dpi=200)\n","plt.plot(epoch_count, training_loss_vgg, 'r--', color= 'navy')\n","plt.plot(epoch_count, val_loss_vgg, '--bo',color= 'orangered', linewidth = '2.5', label='line with marker')\n","plt.legend(['Training Loss', 'Val Loss'])\n","plt.title('Number of epochs & Loss in VGGNET')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.xticks(np.arange(1,21,1))\n","plt.show();"]},{"cell_type":"code","execution_count":null,"id":"a1f940d9","metadata":{"execution":{"iopub.execute_input":"2023-10-18T19:30:07.185122Z","iopub.status.busy":"2023-10-18T19:30:07.181299Z","iopub.status.idle":"2023-10-18T19:30:07.503008Z","shell.execute_reply":"2023-10-18T19:30:07.502284Z"},"papermill":{"duration":0.723296,"end_time":"2023-10-18T19:30:07.504799","exception":false,"start_time":"2023-10-18T19:30:06.781503","status":"completed"},"tags":[],"id":"a1f940d9"},"outputs":[],"source":["plt.figure(figsize=(10,5), dpi=200)\n","plt.plot(epoch_count, training_acc_vgg, 'r--', color= 'navy')\n","plt.plot(epoch_count, val_acc_vgg, '--bo',color= 'orangered', linewidth = '2.5', label='line with marker')\n","plt.legend(['Training Acc', 'Val Acc'])\n","plt.title('Number of epochs & Accuracy in VGGNET')\n","plt.xlabel('Epoch')\n","plt.ylabel('Acc')\n","plt.xticks(np.arange(1,21,1))\n","plt.plot();\n","plt.show();"]},{"cell_type":"markdown","id":"451b4c0d","metadata":{"papermill":{"duration":0.390475,"end_time":"2023-10-18T19:30:08.350110","exception":false,"start_time":"2023-10-18T19:30:07.959635","status":"completed"},"tags":[],"id":"451b4c0d"},"source":["<a id=\"8\"></a>\n","# <p style=\"padding:10px;background-color:#fab72f ;margin:0;color:#ffffff;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:500\">ResNet </p>"]},{"cell_type":"markdown","id":"c562180e","metadata":{"papermill":{"duration":0.405374,"end_time":"2023-10-18T19:30:09.158945","exception":false,"start_time":"2023-10-18T19:30:08.753571","status":"completed"},"tags":[],"id":"c562180e"},"source":["<div style = 'border : 3px solid non; background-color:#f2f2f2 ; ;padding:10px'>\n","\n","\n","* **1. What is ResNet?**\n","\n","  - ResNet (Residual Network) is a deep learning-based model that uses residual connections to improve training performance and accuracy. The architecture of ResNet is designed to solve the problem of vanishing gradients that occurs when training deep neural networks. ResNet introduces the concept of residual blocks, which use skip connections to connect activations of a layer to further layers by skipping some layers in between. This forms a residual block, and ResNets are made by stacking these residual blocks together. The approach behind this network is instead of layers learning the underlying mapping, we allow the network to fit the residual mapping. So, instead of say H (x), initial mapping, let the network fit, F (x) := H (x) - x which gives H (x) := F (x) + x. The skip connection connects activations of a layer to further layers by skipping some layers in between. This results in training a very deep neural network without the problems caused by vanishing/exploding gradient. ResNet is widely used in image classification tasks and has inspired many other CNN architectures that have been developed since its introduction.\n","   \n","\n"]},{"cell_type":"markdown","id":"b7991dea","metadata":{"papermill":{"duration":0.395724,"end_time":"2023-10-18T19:30:09.991027","exception":false,"start_time":"2023-10-18T19:30:09.595303","status":"completed"},"tags":[],"id":"b7991dea"},"source":["<div style = 'border : 3px solid non; background-color:#f2f2f2 ; ;padding:10px'>\n","\n","\n","* **2. Model Structure**\n","\n","  - You can see the structure of VGGNet in the cell below.\n","   \n"]},{"cell_type":"code","execution_count":null,"id":"703477ce","metadata":{"execution":{"iopub.execute_input":"2023-10-18T19:30:10.848138Z","iopub.status.busy":"2023-10-18T19:30:10.846664Z","iopub.status.idle":"2023-10-18T19:30:11.263994Z","shell.execute_reply":"2023-10-18T19:30:11.263427Z"},"papermill":{"duration":0.860485,"end_time":"2023-10-18T19:30:11.301910","exception":false,"start_time":"2023-10-18T19:30:10.441425","status":"completed"},"tags":[],"id":"703477ce"},"outputs":[],"source":["def ResNet34 ():\n","    inp = layers.Input((240, 240, 3))\n","    x = layers.Conv2D(64, 7, 2,padding='valid', activation='relu')(inp)\n","    x = layers.MaxPooling2D(strides=2, padding='same')(x)\n","    x = layers.Conv2D(64, 3, 1,padding='same', activation='relu')(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Conv2D(64, 3, 1,padding='same', activation='relu')(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Conv2D(64, 3, 1,padding='same', activation='relu')(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Conv2D(64, 3, 1,padding='same', activation='relu')(x)\n","    x = layers.BatchNormalization()(x)\n","\n","\n","    x = layers.Conv2D(128, 3, 2,padding='same', activation='relu')(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Conv2D(128, 3, 1,padding='same', activation='relu')(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Conv2D(128, 3, 1,padding='same', activation='relu')(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Conv2D(128, 3, 1,padding='same', activation='relu')(x)\n","    x = layers.BatchNormalization()(x)\n","\n","\n","    x = layers.Conv2D(256, 3, 2,padding='same', activation='relu')(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Conv2D(256, 3, 1,padding='same', activation='relu')(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Conv2D(256, 3, 1,padding='same', activation='relu')(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Conv2D(256, 3, 1,padding='same', activation='relu')(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Conv2D(256, 3, 1,padding='same', activation='relu')(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Conv2D(256, 3, 1,padding='same', activation='relu')(x)\n","    x = layers.BatchNormalization()(x)\n","\n","\n","\n","    x = layers.Conv2D(512, 3, 2,padding='same', activation='relu')(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Conv2D(512, 3, 1,padding='same', activation='relu')(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Conv2D(512, 3, 1,padding='same', activation='relu')(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Flatten()(x)\n","    x = layers.Dense(4096, activation='relu')(x)\n","    x = layers.Dropout(0.5)(x)\n","    x = layers.Dense(4096, activation='relu')(x)\n","    x = layers.Dropout(0.5)(x)\n","    x = layers.Dense(5, activation='softmax')(x)\n","\n","\n","\n","\n","    model_Res = models.Model(inputs=inp, outputs=x)\n","\n","    return model_Res\n","\n","model_Res = ResNet34()\n","model_Res.summary()\n"]},{"cell_type":"code","execution_count":null,"id":"81333443","metadata":{"execution":{"iopub.execute_input":"2023-10-18T19:30:12.111129Z","iopub.status.busy":"2023-10-18T19:30:12.110192Z","iopub.status.idle":"2023-10-18T19:30:12.423837Z","shell.execute_reply":"2023-10-18T19:30:12.423144Z"},"papermill":{"duration":0.721151,"end_time":"2023-10-18T19:30:12.428200","exception":false,"start_time":"2023-10-18T19:30:11.707049","status":"completed"},"tags":[],"id":"81333443"},"outputs":[],"source":["tf.keras.utils.plot_model(\n","    model_Res,\n","    to_file='res_model.png',\n","    show_shapes=True,\n","    show_dtype=False,\n","    show_layer_names=True,\n","    show_layer_activations=True,\n","    dpi=100\n",")"]},{"cell_type":"code","execution_count":null,"id":"6e4987b4","metadata":{"execution":{"iopub.execute_input":"2023-10-18T19:30:13.302531Z","iopub.status.busy":"2023-10-18T19:30:13.301809Z","iopub.status.idle":"2023-10-18T19:30:13.313739Z","shell.execute_reply":"2023-10-18T19:30:13.313180Z"},"papermill":{"duration":0.474982,"end_time":"2023-10-18T19:30:13.315146","exception":false,"start_time":"2023-10-18T19:30:12.840164","status":"completed"},"tags":[],"id":"6e4987b4"},"outputs":[],"source":["model_Res.compile(loss=BinaryCrossentropy(),\n","              optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"id":"ba7690e0","metadata":{"execution":{"iopub.execute_input":"2023-10-18T19:30:14.141811Z","iopub.status.busy":"2023-10-18T19:30:14.140737Z","iopub.status.idle":"2023-10-18T19:52:36.935766Z","shell.execute_reply":"2023-10-18T19:52:36.934666Z"},"papermill":{"duration":1343.208333,"end_time":"2023-10-18T19:52:36.937519","exception":false,"start_time":"2023-10-18T19:30:13.729186","status":"completed"},"tags":[],"id":"ba7690e0"},"outputs":[],"source":["RES_model = model_Res.fit(trainDataset, epochs=20, validation_data=valDataset)"]},{"cell_type":"markdown","id":"86f28750","metadata":{"papermill":{"duration":0.527099,"end_time":"2023-10-18T19:52:37.997566","exception":false,"start_time":"2023-10-18T19:52:37.470467","status":"completed"},"tags":[],"id":"86f28750"},"source":["<div style = 'border : 3px solid non; background-color:#f2f2f2 ; ;padding:10px'>\n","\n","\n","* **3. Output**\n","\n","  - We can plot the output of model for each epoch. For this matter, we can use **.history** and extract the **train and valid loss and accuracy**. Then we can plot both of them and find out the path of learning.\n","   \n"]},{"cell_type":"code","execution_count":null,"id":"d48a6ac5","metadata":{"execution":{"iopub.execute_input":"2023-10-18T19:52:39.119548Z","iopub.status.busy":"2023-10-18T19:52:39.119239Z","iopub.status.idle":"2023-10-18T19:52:39.124466Z","shell.execute_reply":"2023-10-18T19:52:39.123751Z"},"papermill":{"duration":0.539915,"end_time":"2023-10-18T19:52:39.126073","exception":false,"start_time":"2023-10-18T19:52:38.586158","status":"completed"},"tags":[],"id":"d48a6ac5"},"outputs":[],"source":["training_loss_res = RES_model.history['loss']\n","val_loss_res = RES_model.history['val_loss']\n","training_acc_res = RES_model.history['accuracy']\n","val_acc_res = RES_model.history['val_accuracy']"]},{"cell_type":"code","execution_count":null,"id":"ba5f0bdb","metadata":{"execution":{"iopub.execute_input":"2023-10-18T19:52:40.190247Z","iopub.status.busy":"2023-10-18T19:52:40.189393Z","iopub.status.idle":"2023-10-18T19:52:40.497127Z","shell.execute_reply":"2023-10-18T19:52:40.496476Z"},"papermill":{"duration":0.840842,"end_time":"2023-10-18T19:52:40.498633","exception":false,"start_time":"2023-10-18T19:52:39.657791","status":"completed"},"tags":[],"id":"ba5f0bdb"},"outputs":[],"source":["plt.figure(figsize=(10,5), dpi=200)\n","plt.plot(epoch_count, training_loss_res, 'r--', color= 'navy')\n","plt.plot(epoch_count, val_loss_res, '--bo',color= 'orangered', linewidth = '2.5', label='line with marker')\n","plt.legend(['Training Loss', 'Val Loss'])\n","plt.title('Number of epochs & Loss in RESNET34')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.xticks(np.arange(1,21,1))\n","plt.show();"]},{"cell_type":"code","execution_count":null,"id":"59707ff0","metadata":{"execution":{"iopub.execute_input":"2023-10-18T19:52:41.622610Z","iopub.status.busy":"2023-10-18T19:52:41.622300Z","iopub.status.idle":"2023-10-18T19:52:41.934768Z","shell.execute_reply":"2023-10-18T19:52:41.933828Z"},"papermill":{"duration":0.855965,"end_time":"2023-10-18T19:52:41.937418","exception":false,"start_time":"2023-10-18T19:52:41.081453","status":"completed"},"tags":[],"id":"59707ff0"},"outputs":[],"source":["plt.figure(figsize=(10,5), dpi=200)\n","plt.plot(epoch_count, training_acc_res, 'r--', color= 'navy')\n","plt.plot(epoch_count, val_acc_res, '--bo',color= 'orangered', linewidth = '2.5', label='line with marker')\n","plt.legend(['Training Acc', 'Val Acc'])\n","plt.title('Number of epochs & Accuracy in RESNET34')\n","plt.xlabel('Epoch')\n","plt.ylabel('Acc')\n","plt.xticks(np.arange(1,21,1))\n","plt.plot();\n","plt.show();"]},{"cell_type":"markdown","id":"8f2417f4","metadata":{"papermill":{"duration":0.624874,"end_time":"2023-10-18T19:52:43.148929","exception":false,"start_time":"2023-10-18T19:52:42.524055","status":"completed"},"tags":[],"id":"8f2417f4"},"source":["<a id=\"9\"></a>\n","# <p style=\"padding:10px;background-color:#fab72f ;margin:0;color:#ffffff;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:500\">Result </p>"]},{"cell_type":"markdown","id":"9d9fd93e","metadata":{"papermill":{"duration":0.527562,"end_time":"2023-10-18T19:52:44.222249","exception":false,"start_time":"2023-10-18T19:52:43.694687","status":"completed"},"tags":[],"id":"9d9fd93e"},"source":["<div style = 'border : 3px solid non; background-color:#f2f2f2 ; ;padding:10px'>\n","\n","\n","* **1. Comparision of (Acc)s and (Loss)s**\n","\n","  - We can put all of the results in a one plot and comparing the behavior of models outputs.\n","   \n","\n"]},{"cell_type":"code","execution_count":null,"id":"c709b69f","metadata":{"execution":{"iopub.execute_input":"2023-10-18T19:52:45.335967Z","iopub.status.busy":"2023-10-18T19:52:45.335075Z","iopub.status.idle":"2023-10-18T19:52:45.669263Z","shell.execute_reply":"2023-10-18T19:52:45.668532Z"},"papermill":{"duration":0.916633,"end_time":"2023-10-18T19:52:45.670926","exception":false,"start_time":"2023-10-18T19:52:44.754293","status":"completed"},"tags":[],"id":"c709b69f"},"outputs":[],"source":["plt.figure(figsize=(10,5), dpi=200)\n","\n","plt.plot(epoch_count, val_loss_alex, '--bo',color= 'navy',\n","         linewidth = '2.5', label='line with marker')\n","plt.plot(epoch_count, val_loss_vgg, '--bo',color= 'orangered',\n","         linewidth = '2.5', label='line with marker')\n","plt.plot(epoch_count, val_loss_res, '--bo',color= 'black',\n","         linewidth = '2.5', label='line with marker')\n","plt.legend(['AlexNet', 'VGGNet','ResNet'])\n","plt.title('Number of epochs & Loss in All Models')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.xticks(np.arange(1,21,1))\n","plt.show();"]},{"cell_type":"code","execution_count":null,"id":"cb9fe0d7","metadata":{"execution":{"iopub.execute_input":"2023-10-18T19:52:46.754677Z","iopub.status.busy":"2023-10-18T19:52:46.753671Z","iopub.status.idle":"2023-10-18T19:52:47.092226Z","shell.execute_reply":"2023-10-18T19:52:47.091509Z"},"papermill":{"duration":0.879244,"end_time":"2023-10-18T19:52:47.094011","exception":false,"start_time":"2023-10-18T19:52:46.214767","status":"completed"},"tags":[],"id":"cb9fe0d7"},"outputs":[],"source":["plt.figure(figsize=(10,5), dpi=200)\n","\n","plt.plot(epoch_count, val_acc_alex, '--bo',color= 'navy',\n","         linewidth = '2.5', label='line with marker')\n","plt.plot(epoch_count, val_acc_vgg, '--bo',color= 'orangered',\n","         linewidth = '2.5', label='line with marker')\n","plt.plot(epoch_count, val_acc_res, '--bo',color= 'black',\n","         linewidth = '2.5', label='line with marker')\n","plt.legend(['AlexNet', 'VGGNet','ResNet'])\n","plt.title('Number of epochs & Accuracy in All Models')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.xticks(np.arange(1,21,1))\n","plt.show();"]},{"cell_type":"markdown","id":"76700b11","metadata":{"papermill":{"duration":0.54716,"end_time":"2023-10-18T19:52:48.261645","exception":false,"start_time":"2023-10-18T19:52:47.714485","status":"completed"},"tags":[],"id":"76700b11"},"source":["<div style = 'border : 3px solid non; background-color:#f2f2f2 ; ;padding:10px'>\n","\n","\n","* **2. Conclusion**\n","\n","  - As we can see, the result of AlexNet and VGGNet is good but the ResNet model not fitted in a good way. This is what i was mentioned about why we have to use more than one structure models.\n","   \n","\n"]},{"cell_type":"markdown","id":"8d609391","metadata":{"papermill":{"duration":0.540074,"end_time":"2023-10-18T19:52:49.334933","exception":false,"start_time":"2023-10-18T19:52:48.794859","status":"completed"},"tags":[],"id":"8d609391"},"source":["<a id=\"10\"></a>\n","# <p style=\"padding:10px;background-color:#fab72f ;margin:0;color:#ffffff;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:500\">Recommendation Topics </p>"]},{"cell_type":"markdown","id":"4af12f6e","metadata":{"papermill":{"duration":0.58132,"end_time":"2023-10-18T19:52:50.450774","exception":false,"start_time":"2023-10-18T19:52:49.869454","status":"completed"},"tags":[],"id":"4af12f6e"},"source":["<div style = 'border : 3px solid non; background-color:#f2f2f2 ; ;padding:10px'>\n","\n","\n","* **1. How to connect these models to a Camera?**\n","\n","  - To connect deep learning models to a camera with Python, you can use OpenCV, which is an open-source computer vision library. OpenCV provides a simple and easy-to-use interface for capturing video from a camera and processing it in real-time using deep learning models. Here are some steps to get started:\n","Install OpenCV and other required libraries such as TensorFlow, Keras, etc. on your system.\n","Connect your camera to your computer and make sure it is working properly.\n","Write a Python script that captures video from the camera using OpenCV and processes it using your deep learning model.\n","Run the script and test it with your camera.\n","   \n","\n"]},{"cell_type":"markdown","id":"dd56c0b8","metadata":{"papermill":{"duration":0.529662,"end_time":"2023-10-18T19:52:51.510068","exception":false,"start_time":"2023-10-18T19:52:50.980406","status":"completed"},"tags":[],"id":"dd56c0b8"},"source":["<div style = 'border : 3px solid non; background-color:#f2f2f2 ; ;padding:10px'>\n","\n","\n","* **2. How to build a voice alarm system for a Car?**\n","\n","  - To build a voice alarm system for a car with Python, you can use the following steps:\n","Install the required libraries, such as OpenCV, pyttsx3, and threading. These libraries will help you to capture video from a camera, convert text to speech, and run multiple tasks simultaneously.\n","Define a function that will play a voice message when motion is detected. You can use pyttsx3 to create an engine object and set its properties, such as voice, rate, and volume. Then you can use the say method to speak a message and the runAndWait method to wait until the message is finished.\n","Define another function that will detect motion using OpenCV. You can use cv2.VideoCapture to access the camera and cv2.cvtColor to convert the frames to grayscale. Then you can use cv2.absdiff to calculate the difference between two consecutive frames and cv2.threshold to binarize the image. If the number of white pixels in the image is greater than a certain threshold, it means that motion is detected.\n","Use threading to run both functions in parallel. You can create two thread objects and pass the functions as arguments. Then you can use the start method to start the threads and the join method to wait until they finish.\n","Test your code and adjust the parameters as needed. You can change the threshold value, the voice message, or the camera index according to your preference.\n","   \n","\n"]},{"cell_type":"code","source":[],"metadata":{"id":"oHGc6C4jPr3M"},"id":"oHGc6C4jPr3M","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import classification_report\n","\n","# Load dataset\n","data = pd.read_csv(\"driver_behavior.csv\")\n","\n","# Feature selection\n","X = data.drop(\"behavior_label\", axis=1)\n","y = data[\"behavior_label\"]\n","\n","# Split dataset\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Train model\n","model = RandomForestClassifier(n_estimators=100, random_state=42)\n","model.fit(X_train, y_train)\n","\n","# Evaluate model\n","y_pred = model.predict(X_test)\n","print(classification_report(y_test, y_pred))\n"],"metadata":{"id":"LK8cN5YyPrzM"},"id":"LK8cN5YyPrzM","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"080564d8","metadata":{"papermill":{"duration":0.616271,"end_time":"2023-10-18T19:52:52.671334","exception":false,"start_time":"2023-10-18T19:52:52.055063","status":"completed"},"tags":[],"id":"080564d8"},"source":["# <p style=\"padding:10px;background-color:#fab72f ;margin:0;color:#ffffff;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:500\">Warm Wishes</p>"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":5187.750527,"end_time":"2023-10-18T19:52:56.337854","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-10-18T18:26:28.587327","version":"2.4.0"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}